{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNMI Weather Data Processing\n",
    "\n",
    "This notebook processes meteorological data from the Royal Netherlands Meteorological Institute (KNMI) to support the Gelderse Poort rewilding and NDVI productivity research project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T21:53:25.562016Z",
     "start_time": "2025-02-25T21:53:24.649108Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# setting of directory\n",
    "output_dir = 'weatherdata_postprocessing'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = 'weatherdata_raw/weather.csv'\n",
    "\n",
    "raw_data = pd.read_csv(raw_data_path, comment='#',  sep=',', header=0)\n",
    "\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform of the date       \n",
    "raw_data['date'] = pd.to_datetime(raw_data['YYYYMMDD'], format='%Y%m%d')\n",
    "\n",
    "# transform of the unit and rename the column\n",
    "processed_data = pd.DataFrame()\n",
    "processed_data['date'] = raw_data['date']\n",
    "processed_data['station_id'] = raw_data['STN']\n",
    "\n",
    "# add the station name (according to the station id)\n",
    "station_names = {275: 'Deelen', 391: 'Arcen'}\n",
    "processed_data['station_name'] = processed_data['station_id'].map(station_names)\n",
    "\n",
    "# transform of the temperature unit (0.1°C to °C)\n",
    "processed_data['temp_mean_c'] = raw_data['TG'] / 10.0\n",
    "processed_data['temp_min_c'] = raw_data['TN'] / 10.0\n",
    "processed_data['temp_max_c'] = raw_data['TX'] / 10.0\n",
    "\n",
    "# transform of the precipitation unit (0.1mm to mm)\n",
    "processed_data['precip_mm'  ] = raw_data['RH'] / 10.0\n",
    "# handle the special value (-1 represents <0.05mm)\n",
    "processed_data.loc[raw_data['RH'] == -1, 'precip_mm'] = 0.025\n",
    "\n",
    "# other datasets\n",
    "processed_data['radiation_j_cm2'] = raw_data['Q']\n",
    "processed_data['rel_humidity_pct'] = raw_data['UG']\n",
    "processed_data['et0_mm'] = raw_data['EV24'] / 10.0\n",
    "\n",
    "# calculate the water balance\n",
    "processed_data['water_balance_mm'] = processed_data['precip_mm'] - processed_data['et0_mm']\n",
    "\n",
    "# examine the data  \n",
    "print(\"exmaine the data:\")\n",
    "# check the negative value\n",
    "negative_values = {col: processed_data[col][processed_data[col] < 0].count() \n",
    "                  for col in processed_data.select_dtypes(include=[np.number]).columns}\n",
    "print(\"negative value statistics:\", {k: v for k, v in negative_values.items() if v > 0})\n",
    "\n",
    "# check the missing value\n",
    "missing_values = processed_data.isnull().sum()\n",
    "print(\"missing value statistics:\", missing_values[missing_values > 0] if any(missing_values > 0) else \"no missing value\")\n",
    "\n",
    "# check the outliers\n",
    "numeric_cols = processed_data.select_dtypes(include=[np.number]).columns\n",
    "outliers = {}\n",
    "for col in numeric_cols:\n",
    "    mean = processed_data[col].mean()\n",
    "    std = processed_data[col].std()\n",
    "    outlier_count = processed_data[processed_data[col] > mean + 3*std].shape[0]\n",
    "    if outlier_count > 0:\n",
    "        outliers[col] = outlier_count\n",
    "print(\"negative value statistics (>3σ):\", outliers if outliers else \"no obvious outliers\")\n",
    "\n",
    "# view the processed data\n",
    "print(\"\\nprocessed data preview:\")\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. missing value\n",
    "# check the missing value\n",
    "missing_rows = processed_data[processed_data.isnull().any(axis=1)]\n",
    "print(f\"include missing value rows: {len(missing_rows)}\")\n",
    "if len(missing_rows) > 0:\n",
    "    print(\"missing value rows information:\")\n",
    "    print(missing_rows[['date', 'station_id', 'station_name']])\n",
    "\n",
    "    # using linear interpolation to fill the missing value\n",
    "    print(\"using linear interpolation to fill the missing value...\")\n",
    "    # group by station to interpolate\n",
    "    for station_id in processed_data['station_id'].unique():\n",
    "        station_mask = processed_data['station_id'] == station_id\n",
    "        processed_data.loc[station_mask, 'radiation_j_cm2'] = processed_data.loc[station_mask, 'radiation_j_cm2'].interpolate(method='linear')\n",
    "        processed_data.loc[station_mask, 'et0_mm'] = processed_data.loc[station_mask, 'et0_mm'].interpolate(method='linear')\n",
    "        \n",
    "    # recalculate the water balance\n",
    "    processed_data['water_balance_mm'] = processed_data['precip_mm'] - processed_data['et0_mm']\n",
    "    \n",
    "    # check if there are still missing values\n",
    "    remaining_missing = processed_data.isnull().sum()\n",
    "    print(\"remaining missing value:\", remaining_missing[remaining_missing > 0] if any(remaining_missing > 0) else \"all filled\")\n",
    "\n",
    "# 2. abnormal value processing\n",
    "# negative temperature is normal meteorological phenomenon, it is reserved.\n",
    "print(\"\\nnegative temperature is normal meteorological phenomenon, it is reserved.\")\n",
    "print(f\"the minimum temperature range: {processed_data['temp_min_c'].min():.1f}°C  to {processed_data['temp_min_c'].max():.1f}°C\")\n",
    "\n",
    "# for extremely extreme precipitation values, we can check and visualize\n",
    "extreme_precip = processed_data[processed_data['precip_mm'] > processed_data['precip_mm'].mean() + 3*processed_data['precip_mm'].std()]\n",
    "print(f\"\\nextreme precipitation records: {len(extreme_precip)}\")\n",
    "if len(extreme_precip) > 0:\n",
    "    print(\"extreme precipitation value range:\", f\"{extreme_precip['precip_mm'].min():.1f}mm  to {extreme_precip['precip_mm'].max():.1f}mm\")\n",
    "    print(\"time distribution:\")\n",
    "    print(extreme_precip['date'].dt.year.value_counts().sort_index())\n",
    "    \n",
    "    # check the most extreme values\n",
    "    print(\"\\nhighest precipitation records:\")\n",
    "    print(extreme_precip.sort_values('precip_mm', ascending=False).head(5)[['date', 'station_name', 'precip_mm']])\n",
    "    \n",
    "    # for extremely extreme values (e.g. >200mm/day), we can consider marking but not modifying\n",
    "    extremely_high_precip = processed_data[processed_data['precip_mm'] > 200]\n",
    "    if len(extremely_high_precip) > 0:\n",
    "        print(f\"\\nextremely high precipitation (>200mm): {len(extremely_high_precip)} records\")\n",
    "        print(extremely_high_precip[['date', 'station_name', 'precip_mm']])\n",
    "        \n",
    "        # add a flag column but not change the original value\n",
    "        processed_data['precip_extreme_flag'] = processed_data['precip_mm'] > 200\n",
    "    else:\n",
    "        processed_data['precip_extreme_flag'] = False\n",
    "\n",
    "# recalculate the outlier statistics\n",
    "print(\"\\nprocessed outlier statistics:\")\n",
    "outliers = {}\n",
    "for col in numeric_cols:\n",
    "    if col != 'precip_extreme_flag':  # skip the flag column\n",
    "        mean = processed_data[col].mean()\n",
    "        std = processed_data[col].std()\n",
    "        outlier_count = processed_data[processed_data[col] > mean + 3*std].shape[0]\n",
    "        if outlier_count > 0:\n",
    "            outliers[col] = outlier_count\n",
    "print(\"negative value statistics (>3σ):\", outliers if outliers else \"no obvious outliers\")\n",
    "\n",
    "# view the processed data\n",
    "print(\"\\nprocessed data preview:\")\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Cumulative Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is sorted by date\n",
    "processed_data = processed_data.sort_values(['station_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Calculate rolling cumulative indicators\n",
    "result_data = processed_data.copy()\n",
    "\n",
    "# Calculate separately for each station\n",
    "for station_id in result_data['station_id'].unique():\n",
    "    mask = result_data['station_id'] == station_id\n",
    "    \n",
    "    # Calculate rolling cumulative precipitation\n",
    "    result_data.loc[mask, 'precip_7day_mm'] = result_data.loc[mask, 'precip_mm'].rolling(window=7, min_periods=5).sum()\n",
    "    result_data.loc[mask, 'precip_30day_mm'] = result_data.loc[mask, 'precip_mm'].rolling(window=30, min_periods=25).sum()\n",
    "    result_data.loc[mask, 'precip_90day_mm'] = result_data.loc[mask, 'precip_mm'].rolling(window=90, min_periods=75).sum()\n",
    "    \n",
    "    # Calculate rolling water balance\n",
    "    result_data.loc[mask, 'water_balance_7day_mm'] = result_data.loc[mask, 'water_balance_mm'].rolling(window=7, min_periods=5).sum()\n",
    "    result_data.loc[mask, 'water_balance_30day_mm'] = result_data.loc[mask, 'water_balance_mm'].rolling(window=30, min_periods=25).sum()\n",
    "\n",
    "# View calculated data\n",
    "print(\"\\nData example after adding cumulative indicators:\")\n",
    "print(result_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Drought Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate simplified drought indicators for each station separately\n",
    "final_data = result_data.copy()\n",
    "\n",
    "for station_id in final_data['station_id'].unique():\n",
    "    station_mask = final_data['station_id'] == station_id\n",
    "    station_data = final_data[station_mask].copy()\n",
    "    \n",
    "    # Calculate historical mean and standard deviation for each calendar day\n",
    "    station_data['month_day'] = station_data['date'].dt.strftime('%m-%d')\n",
    "    \n",
    "    # Calculate statistical values for each calendar day\n",
    "    agg_data = station_data.groupby('month_day').agg({\n",
    "        'precip_30day_mm': ['mean', 'std'],\n",
    "        'water_balance_30day_mm': ['mean', 'std']\n",
    "    })\n",
    "    \n",
    "    # Flatten multi-level index\n",
    "    agg_data.columns = ['_'.join(col) for col in agg_data.columns]\n",
    "    \n",
    "    # Merge statistical values back to original data\n",
    "    station_data = station_data.merge(agg_data, left_on='month_day', right_index=True)\n",
    "    \n",
    "    # Calculate simplified SPI and SPEI (based on Z-scores)\n",
    "    station_data['simplified_spi_1month'] = np.nan\n",
    "    station_data['simplified_spei_1month'] = np.nan\n",
    "    \n",
    "    # Only calculate standardized indices where standard deviation > 0\n",
    "    valid_mask = station_data['precip_30day_mm_std'] > 0\n",
    "    station_data.loc[valid_mask, 'simplified_spi_1month'] = (\n",
    "        station_data.loc[valid_mask, 'precip_30day_mm'] - \n",
    "        station_data.loc[valid_mask, 'precip_30day_mm_mean']\n",
    "    ) / station_data.loc[valid_mask, 'precip_30day_mm_std']\n",
    "    \n",
    "    valid_mask = station_data['water_balance_30day_mm_std'] > 0\n",
    "    station_data.loc[valid_mask, 'simplified_spei_1month'] = (\n",
    "        station_data.loc[valid_mask, 'water_balance_30day_mm'] - \n",
    "        station_data.loc[valid_mask, 'water_balance_30day_mm_mean']\n",
    "    ) / station_data.loc[valid_mask, 'water_balance_30day_mm_std']\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    keep_columns = final_data.columns.tolist() + ['simplified_spi_1month', 'simplified_spei_1month']\n",
    "    station_data = station_data[keep_columns]\n",
    "    \n",
    "    # Update final data\n",
    "    final_data.loc[station_mask] = station_data\n",
    "\n",
    "# View final data\n",
    "print(\"\\nFinal processed data example:\")\n",
    "print(final_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Identify Extreme Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to identify extreme events\n",
    "def identify_extreme_events(df, spi_threshold=-1.5, spei_threshold=-1.5, precip_threshold=30):\n",
    "    \"\"\"Identify extreme drought and flood events\"\"\"\n",
    "    extreme_events = {'drought': [], 'flood': []}\n",
    "    \n",
    "    for station_id, station_data in df.groupby('station_id'):\n",
    "        station_name = station_data['station_name'].iloc[0]\n",
    "        \n",
    "        # Identify drought events - consecutive 14+ days with low SPI or SPEI\n",
    "        drought_mask = (station_data['simplified_spi_1month'] <= spi_threshold) | \\\n",
    "                      (station_data['simplified_spei_1month'] <= spei_threshold)\n",
    "        \n",
    "        if drought_mask.any():\n",
    "            # Find drought days\n",
    "            drought_days = station_data[drought_mask][['date', 'simplified_spi_1month', 'simplified_spei_1month']]\n",
    "            \n",
    "            # Determine start and end dates of drought events (simplified version)\n",
    "            if len(drought_days) >= 14:\n",
    "                start_date = drought_days['date'].min()\n",
    "                end_date = drought_days['date'].max()\n",
    "                duration = (end_date - start_date).days + 1\n",
    "                \n",
    "                # Calculate event severity\n",
    "                min_spi = drought_days['simplified_spi_1month'].min()\n",
    "                min_spei = drought_days['simplified_spei_1month'].min()\n",
    "                severity = 'Severe' if (min_spi < -2.0 or min_spei < -2.0) else 'Moderate'\n",
    "                \n",
    "                extreme_events['drought'].append({\n",
    "                    'station_id': station_id,\n",
    "                    'station_name': station_name,\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'duration_days': duration,\n",
    "                    'min_spi': min_spi,\n",
    "                    'min_spei': min_spei,\n",
    "                    'severity': severity\n",
    "                })\n",
    "        \n",
    "        # Identify flood events - daily precipitation exceeding threshold\n",
    "        flood_mask = station_data['precip_mm'] >= precip_threshold\n",
    "        \n",
    "        if flood_mask.any():\n",
    "            flood_days = station_data[flood_mask][['date', 'precip_mm']]\n",
    "            \n",
    "            # Simplified version: treat each day exceeding threshold as a flood event\n",
    "            for _, row in flood_days.iterrows():\n",
    "                extreme_events['flood'].append({\n",
    "                    'station_id': station_id,\n",
    "                    'station_name': station_name,\n",
    "                    'date': row['date'],\n",
    "                    'precipitation_mm': row['precip_mm'],\n",
    "                    'severity': 'Severe' if row['precip_mm'] >= 50 else 'Moderate'\n",
    "                })\n",
    "    \n",
    "    return extreme_events\n",
    "\n",
    "# Identify extreme events\n",
    "extreme_events = identify_extreme_events(final_data)\n",
    "\n",
    "# View identified extreme events\n",
    "print(\"\\nIdentified drought events:\")\n",
    "if extreme_events['drought']:\n",
    "    for event in extreme_events['drought']:\n",
    "        print(f\"Station: {event['station_name']}, Start: {event['start_date'].date()}, End: {event['end_date'].date()}, Duration: {event['duration_days']} days, Severity: {event['severity']}\")\n",
    "else:\n",
    "    print(\"No drought events identified\")\n",
    "    \n",
    "print(\"\\nIdentified flood events:\")\n",
    "if extreme_events['flood']:\n",
    "    for event in extreme_events['flood'][:5]:  # Show only first 5\n",
    "        print(f\"Station: {event['station_name']}, Date: {event['date'].date()}, Precipitation: {event['precipitation_mm']}mm, Severity: {event['severity']}\")\n",
    "    if len(extreme_events['flood']) > 5:\n",
    "        print(f\"... and {len(extreme_events['flood'])-5} other flood events\")\n",
    "else:\n",
    "    print(\"No flood events identified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed meteorological data\n",
    "output_file = os.path.join(output_dir, 'processed_weather_data.csv')\n",
    "final_data.to_csv(output_file, index=False)\n",
    "print(f\"\\nProcessed meteorological data saved to: {output_file}\")\n",
    "\n",
    "# Save extreme event information\n",
    "if extreme_events['drought']:\n",
    "    drought_df = pd.DataFrame(extreme_events['drought'])\n",
    "    drought_file = os.path.join(output_dir, 'drought_events.csv')\n",
    "    drought_df.to_csv(drought_file, index=False)\n",
    "    print(f\"Drought event information saved to: {drought_file}\")\n",
    "\n",
    "if extreme_events['flood']:\n",
    "    flood_df = pd.DataFrame(extreme_events['flood'])\n",
    "    flood_file = os.path.join(output_dir, 'flood_events.csv')\n",
    "    flood_df.to_csv(flood_file, index=False)\n",
    "    print(f\"Flood event information saved to: {flood_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precipitation and drought indicator time series\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot separate graphs for each station\n",
    "for station_id, station_data in final_data.groupby('station_id'):\n",
    "    plt.subplot(2, 1, 1 if station_id == 275 else 2)\n",
    "    \n",
    "    # Plot monthly precipitation\n",
    "    plt.plot(station_data['date'], station_data['precip_30day_mm'], 'b-', label='30-day cumulative precipitation (mm)')\n",
    "    \n",
    "    # Plot drought indicators\n",
    "    plt.plot(station_data['date'], station_data['simplified_spi_1month'], 'r-', label='Simplified SPI (1-month)')\n",
    "    \n",
    "    # Add drought and wet threshold lines\n",
    "    plt.axhline(y=-1.5, color='r', linestyle='--', alpha=0.7)\n",
    "    plt.axhline(y=1.5, color='b', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.title(f\"{station_data['station_name'].iloc[0]} Station Precipitation and Drought Indicators\")\n",
    "    plt.ylabel('Precipitation (mm) / SPI Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_file = os.path.join(output_dir, 'precipitation_drought_timeseries.png')\n",
    "plt.savefig(fig_file, dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\nPrecipitation and drought indicator time series plot saved to: {fig_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
